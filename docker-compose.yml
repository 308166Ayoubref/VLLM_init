version: "3.8"

services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    volumes:
      - /mnt/hugging-face-models/mistral/mistral-small-instruct-24b:/model
    ports:
      - "8000:8000"
    command: >
      --model /model
      --served-model-name mistral-small
      --tokenizer-mode mistral
      --config-format mistral
      --load-format mistral
      --tool-call-parser mistral
      --enable-auto-tool-choice
      --limit-mm-per-prompt 'image=10'
      --tensor-parallel-size 4
      --dtype float16
      --max-parallel-loading-workers 8
      --max-model-len 128000
      --gpu-memory-utilization 0.97
      --max-num-batched-tokens 8192
      --enforce-eager
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  image-server:
    image: tiangolo/uvicorn-gunicorn-fastapi:python3.11
    volumes:
      - /home/ubuntu/rag/output_images:/app/images
      - ./image.py:/app/main.py
    ports:
      - "8100:8100"
    environment:
      - PORT=8100

version: "3.8"
services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    volumes:
      - /mnt/hugging-face-models/mistral/mistral-small-awq-24b:/model
      #- /mnt/hugging-face-models/deepseek/R1-14b-awq:/model
    ports:
      - "8000:8000"
    command: >
      --model /model
      --served-model-name deepseek
      --quantization awq
      
      --dtype float16
      --max-model-len 129700
      --gpu-memory-utilization 0.97
      --config-format hf  
      --max-num-seqs 2
      --max-num-batched-tokens 8192
      --num-lookahead-slots 4
      --enforce-eager
          



    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.rag
    ports:
      - "8501:8501"
    depends_on:
      - vllm
    environment:
      - MODEL_SERVER_URL=http://vllm:8000/v1
    command: streamlit run affichage5.py --server.address=0.0.0.0 --server.port=8501